---
layout: post  
title:  "中文搜索引擎的研究(转)"
date:  2006-06-03 09:07:00
author: Pickle Cai  
categories: Reprinted  
keywords: 
description:   
tags:	pickle   
cover:  "/assets/cover.jpg"  

---

目前搜索引擎的应用越来越广，是网民的上网必备工具。



在中国使用广泛的搜索引擎主要有：baidu 、google 、中搜、 北大天网 、一搜、 搜狗， 还有一些专业的搜索，比如海量做的音乐搜索 http://www.1234567.com ，还有 西祠胡同 的创始人 做的 http://www.pagou.com ，这些都是做的挺不错。由此可见，搜索引擎的市场还是非常庞大的。尤其是baidu的成功上市，给业界很大的鼓舞。



目前的主要搜索引擎的模式都是，用户输入一些关键字或者句子，无论是那种，搜索引擎都会首先对用户的输入进行分词，这样可以增加搜索结果的准确性，这是和普通数据库搜索的不同点（普通的数据库搜索，只是简单的用 like %关键字%），然后搜索引擎去海量的索引库去查找这些和用户输入相关的信息，显示的结果会包含网页的相关摘要。



中文搜索引擎相关的技术包含：网络蜘蛛，中文分词，索引库，网页摘要的提取，网页相似度，信息的分类。 



      1。网络蜘蛛 网络蛛蛛是指对浩瀚网络抓取信息的程序，他们往往是多线程，不分昼夜的抓取网络信息，同时要防止对某个站点抓取过快，导致信息提供方服务器过载。



          网络蜘蛛的基本原理：先从一个起始页面（建议从yahoo中文目录或者dmoz中文目录）开始抓取，获取此页面内容，摘要，然后提取页面所有连接，蜘蛛接着抓取这些连接，一直源源不断的抓取。这些只是基本原理，实际应用要复杂很多，你可以试着自己写一个蜘蛛，我曾经用PHP写过（PHP不能多线程，缺陷。）



      2。中文分词 中文分词一直是中文搜索引擎的关键点，中文不同英文，英文每个单词是用空格分开，而中文一个句子往往是一些词的连结，没有分割符，人可以很容易的看懂句子的意思，但是计算机很难开懂。



           目前我了解的中文分词方法（据说有老外的不用词典的中文分词方法），几乎都是有自己的中文词典，分词时去词典匹配，达到分词目的，分词的好坏，和词典关系很大。你可以看我上篇文章，是用PHP写的中文分词方法。



           目前很多高校语言学的硕士论文都写的这个——baidu用的自己开发分词方法，google用的第3方的分词方法。 海量中文分词挺不错，不过是商业的。   猎兔的中文分词方法也不错，不过是.so的，无法研究 



        3。索引库 搜索引擎都不会用已经成型的数据库系统，他们是自己开发的类似数据库功能的东西。 



              搜索引擎需要保存大量网页信息，快照，关键字索引（建议应该也保存网页的截图，我在研究中），所以数据量特别大。  



        4。网页摘要的提取   网页摘要是指对某个网页信息的总结（初中语文课，老师经常让总结文章的中心思想，就这个意思，我最怕老师提问让我总结，人总结都这么难，现在让计算机总结，天啦），搜索引擎搜索结果里，往往会有网页标题下面，会有些介绍，让搜索者很容易的发现此文章是不是想要的信息。



        5。网页相似度   网上经常有很多内容一样的网站，比如说同一条新闻，各大门户网站都会发布，它们的新闻内容都是一样的。还有一些个人网站，尤其是偷别人网站资料的网站，和别人网站搞的一模一样（我搞过，在此ps下自己），这样的网站毫无意义，搜索引擎会自动区分，降低其权值（baidu最狠，直接封站，我尝试过）。



             目前我研究的计算网页相似度的几种方法如下：



             1) 根据网页摘要来比较，如果多个网页摘要的md5值一样，证明这些网页有很高的相似性 



             2) 根据网页出现关键词，按照词频排序，可以取N个词频高的，如果md5值一样，证明这些网页有很高的相似性。google baidu 的新闻，是对此技术的应用。目前很多高校的数据挖掘专业的研究生论文都写的这个   



            6。信息的自动分类   网络的信息实在是太庞大了，如何对其进行分类，是搜索引擎面临的难题。要让计算机对数据自动分类，先要对计算机程序进行培训，目前我正在研究中   爬狗做的不错。  



注： 以上均为柳志强原创



		    


